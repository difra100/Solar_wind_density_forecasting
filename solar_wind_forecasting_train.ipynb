{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A CNN-LSTM framework for the solar wind density forecasting\n",
    "## ConvLSTM training\n",
    "In this notebook we train a ConvLSTM network to predict solar wind densities (electrons + protons)\n",
    "\n",
    "\n",
    "#### Notebook Contributors\n",
    "* Andrea Giuseppe Di Francesco -- email: difrancesco.1836928@studenti.uniroma1.it\n",
    "* Massimo Coppotelli -- email: coppotelli.1705325@studenti.uniroma1.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install matplotlib\n",
    "# !pip install torchvision\n",
    "# !pip install wandb\n",
    "# !pip3 install pytorch-lightning==1.5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "# Personal files\n",
    "from convlstm import *\n",
    "from utils import *\n",
    "from init import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdifra00\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "if wb:\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dataset = pd.read_csv('./datasets/wind_dataset_1d_res.csv', index_col = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch lightning code \n",
    "- Need of a collate function to preprocess data: sun_images are expressed as lists in a json files, thus we need a preprocessing before feed them into the ConvLSTM.\n",
    "- Then we define a Lightning DataModule, and finally the Lightining Module for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pl_Dataset_(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,  dataset, bs):\n",
    "      \n",
    "\n",
    "      self.train_set = dataset.loc[0:round(len(dataset)*train_split)]\n",
    "      self.val_set = dataset.loc[round(len(dataset)*train_split)+1: round(len(dataset)*train_split) + round(len(dataset)*val_split)]\n",
    "\n",
    "      self.bs = bs\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = DataSet(self.train_set)\n",
    "        elif stage == 'test':\n",
    "            self.val_dataset = DataSet(self.val_set)\n",
    "            \n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.bs, shuffle = True, collate_fn = collate) #, transform = transform)  Transformation was already made during the processing\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader(self.val_dataset, batch_size = self.bs, shuffle = False, collate_fn = collate) #, transform = transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SettingData = pl_Dataset_(wind_dataset, batch_size)\n",
    "\n",
    "SettingData.setup('fit')\n",
    "SettingData.setup('test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for stronzo in SettingData.train_dataloader():\n",
    "    input = stronzo[0]\n",
    "    break\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Save_Model_(Callback):\n",
    "\n",
    "  ''' This Callback is fundamental to save the model, and also its performances over time. It considers also if we want to save the model, given a path, or if we do not want to do so. '''\n",
    "  def __init__(self, path, experiment_name):\n",
    "    self.path = path\n",
    "    self.exp_name = experiment_name\n",
    "\n",
    "  def on_train_epoch_end(self, trainer, pl_module):\n",
    "    model_weights = pl_module.model.state_dict()\n",
    "    self.path = self.path + self.exp_name +'.pt'\n",
    "    save_model({'model_state':model_weights}, self.path)\n",
    "\n",
    "class Get_Metrics(Callback):\n",
    "\n",
    "  def __init__(self):\n",
    "    r = 4\n",
    " \n",
    "  def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "\n",
    "    mean_train_loss = sum(pl_module.loss_train)/len(pl_module.loss_train)\n",
    "    \n",
    "    mean_test_loss = sum(pl_module.loss_test)/len(pl_module.loss_test)\n",
    "\n",
    "    pl_module.log(name = 'Loss on train', value = mean_train_loss)\n",
    "    pl_module.log(name = 'Loss on test', value = mean_test_loss)\n",
    "\n",
    "    R_prt = torch.corrcoef(pl_module.R_prt)[0,1].item()\n",
    "    R_elc = torch.corrcoef(pl_module.R_elc)[0,1].item()\n",
    "\n",
    "    pl_module.log(name = 'Electron Density correlation (R)', value = R_elc)\n",
    "    pl_module.log(name = 'Proton Density correlation (R)', value = R_prt)\n",
    "\n",
    "\n",
    "    pl_module.loss_train = []\n",
    "    pl_module.loss_test = []\n",
    "    pl_module.R_elc = torch.tensor([[]])  \n",
    "    pl_module.R_prt = torch.tensor([[]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, experiment_name):\n",
    "        super().__init__()\n",
    "        self.model = HeliosNet(n_channels, n_hidden_channels, kernel_size, batch_first, bias)\n",
    "        self.MSE = nn.MSELoss(reduction = 'mean')\n",
    "        self.training_hp = training_hp\n",
    "        self.loss_train = []\n",
    "        self.loss_test = []\n",
    "        self.R_elc = torch.tensor([[]])   #\n",
    "        self.R_prt = torch.tensor([[]])\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        input = batch[0]     # Input of size (batch_size x timesteps_length x n_channels x height x width)\n",
    "        targets = batch[1]   # Output of size (batch_size x 2).\n",
    "\n",
    "        output = self.model(input)\n",
    "        \n",
    "        loss = self.MSE(output, targets)\n",
    "\n",
    "        self.loss_train.append(loss.item())\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        input = batch[0]     # Input of size (batch_size x timesteps_length x n_channels x height x width)\n",
    "        targets = batch[1]   # Output of size (batch_size x 2).\n",
    "\n",
    "        output = self.model(input)\n",
    "        \n",
    "        loss = self.MSE(output, targets)\n",
    "        self.loss_test.append(loss.item())\n",
    "\n",
    "        if self.R_prt.shape[1] != 0:\n",
    "            cat_prt = torch.cat((output[:,0].unsqueeze(0), targets[:, 0].unsqueeze(0)), dim = 0)\n",
    "            cat_elc = torch.cat((output[:,1].unsqueeze(0), targets[:, 1].unsqueeze(0)), dim = 0)\n",
    "\n",
    "            self.R_prt = torch.cat((self.R_prt, cat_prt), dim = 1)\n",
    "            self.R_elc = torch.cat((self.R_elc, cat_elc), dim = 1)\n",
    "        else:\n",
    "            self.R_prt = torch.cat((output[:,0].unsqueeze(0), targets[:, 0].unsqueeze(0)), dim = 0)\n",
    "            self.R_elc = torch.cat((output[:,1].unsqueeze(0), targets[:, 1].unsqueeze(0)), dim = 0)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.training_hp['lr'], weight_decay = self.training_hp['wd'])\n",
    "\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'prova'\n",
    "load = False\n",
    "save = True\n",
    "\n",
    "path = './models/'+exp_name+'.pt'\n",
    "\n",
    "\n",
    "pl_training_MDL = TrainingModule(experiment_name = exp_name)\n",
    "\n",
    "if load:\n",
    "    \n",
    "    load_model(path, pl_training_MDL, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "num_gpu = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "\n",
    "if wb:\n",
    "    # initialise the wandb logger and name your wandb project\n",
    "    wandb_logger = WandbLogger(project=project_name, name = exp_name, config = training_hp, entity = 'small_sunbirds')\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs = training_hp['epochs'],  # maximum number of epochs.\n",
    "        gpus=num_gpu,  # the number of gpus we have at our disposal.\n",
    "        default_root_dir=\"\", logger = wandb_logger, callbacks = [Get_Metrics()]\n",
    "    )\n",
    "    if save:\n",
    "        trainer.callbacks.append(Save_Model_(path, exp_name))\n",
    "\n",
    "else:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs = training_hp['epochs'],  # maximum number of epochs.\n",
    "        gpus=num_gpu,  # the number of gpus we have at our disposal.\n",
    "        default_root_dir=\"\", callbacks = [Get_Metrics()] \n",
    "    )\n",
    "    if save:\n",
    "        trainer.callbacks.append(Save_Model_(path, exp_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdifra00\u001b[0m (\u001b[33msmall_sunbirds\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peppe/Desktop/Università/magistrale/Solar_wind_density_forecasting/wandb/run-20230107_212305-mda9wvlx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/small_sunbirds/A%20CNN-LSTM%20framework%20for%20the%20solar%20wind%20density%20forecasting/runs/mda9wvlx\" target=\"_blank\">prova</a></strong> to <a href=\"https://wandb.ai/small_sunbirds/A%20CNN-LSTM%20framework%20for%20the%20solar%20wind%20density%20forecasting\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | HeliosNet | 18.9 M\n",
      "1 | MSE   | MSELoss   | 0     \n",
      "------------------------------------\n",
      "18.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 M    Total params\n",
      "75.543    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppe/miniconda3/envs/my_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 104/104 [00:30<00:00,  3.36it/s, loss=8.15, v_num=wvlx]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Electron Density correlation (R)</td><td>▁████</td></tr><tr><td>Loss on test</td><td>▇█▃▁▁</td></tr><tr><td>Loss on train</td><td>▆█▆▁▁</td></tr><tr><td>Proton Density correlation (R)</td><td>▁████</td></tr><tr><td>epoch</td><td>▁▁▃▃▅▅▆▆██</td></tr><tr><td>test loss</td><td>██▃▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▃▃▅▅▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Electron Density correlation (R)</td><td>0.66126</td></tr><tr><td>Loss on test</td><td>10.57732</td></tr><tr><td>Loss on train</td><td>6.12063</td></tr><tr><td>Proton Density correlation (R)</td><td>0.59727</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>test loss</td><td>10.57732</td></tr><tr><td>trainer/global_step</td><td>389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">prova</strong>: <a href=\"https://wandb.ai/small_sunbirds/A%20CNN-LSTM%20framework%20for%20the%20solar%20wind%20density%20forecasting/runs/mda9wvlx\" target=\"_blank\">https://wandb.ai/small_sunbirds/A%20CNN-LSTM%20framework%20for%20the%20solar%20wind%20density%20forecasting/runs/mda9wvlx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230107_212305-mda9wvlx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model = pl_training_MDL, datamodule = SettingData)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty1 = torch.tensor([])\n",
    "\n",
    "\n",
    "train = torch.randn((16, 2))\n",
    "target = torch.randn((16, 2))\n",
    "\n",
    "cat = torch.cat((train[:,0].unsqueeze(0), target[:, 0].unsqueeze(0)), dim = 0)\n",
    "\n",
    "empty1 = torch.cat((empty1, cat), dim = 1)\n",
    "empty1.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[]]).shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb5445bfbf8972ec5f7b2dac98ed41742e280e56e41d5d2d5df357ace3791bc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
